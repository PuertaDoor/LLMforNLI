{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6ae96018-2e35-482c-b45b-34c86e6c959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import gc\n",
    "from transformers import AutoTokenizer, MT5ForSequenceClassification, BitsAndBytesConfig, Trainer, TrainingArguments, T5ForSequenceClassification\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2e904de8-36d2-47c0-a2c3-b66b57722c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54cd13a3f5a436caf710ffa33033aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google/flan-t5-xl and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "model_id = \"google/flan-t5-xl\"\n",
    "#model_id = \"philschmid/flan-t5-xxl-sharded-fp16\"\n",
    "#model_id = \"google/mt5-xl\"\n",
    "\n",
    "id2label = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "label2id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "model = T5ForSequenceClassification.from_pretrained(model_id, num_labels=3, id2label=id2label, label2id=label2id, device_map=\"auto\")\n",
    "#model = MT5ForSequenceClassification.from_pretrained(model_id, num_labels=3, device_map=\"auto\", return_dict=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7aa66f75-2dc2-4a7a-b8fb-23a0ea815883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "afd4453d-4522-4a67-9cad-f78807e36901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 155197443 || all params: 2943358982 || trainable%: 5.272800360034371\n"
     ]
    }
   ],
   "source": [
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    r=256,\n",
    "    lora_alpha=256,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    modules_to_save=['classification_head']\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "da3f7a07-ac22-4788-b112-3469b02dedf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_dataset(\"ankitkupadhyay/XNLI\")[\"train\"]\n",
    "# data = load_dataset(\"Gameselo/monolingual-wideNLI\")\n",
    "anli_r1 = load_dataset('anli', split='train_r1')\n",
    "anli_r2 = load_dataset('anli', split='train_r2')\n",
    "anli_r3 = load_dataset('anli', split='train_r3')\n",
    "\n",
    "# Concaténer les datasets\n",
    "data = concatenate_datasets([anli_r1, anli_r2, anli_r3])\n",
    "\n",
    "data = data.shuffle(seed=1234)  # Shuffle dataset here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d8928c-ba92-432b-b178-d6fb948f32b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF USING XNLI\n",
    "\n",
    "data = data.train_test_split(test_size=0.1)\n",
    "train_data = data[\"train\"]\n",
    "test_data = data[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e35151-d9aa-447a-80a2-8fe25cb095a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF USING MONOLINGUAL-WIDENLI\n",
    "\n",
    "anli_r1 = load_dataset('anli', split='dev_r1')\n",
    "anli_r2 = load_dataset('anli', split='dev_r2')\n",
    "anli_r3 = load_dataset('anli', split='dev_r3')\n",
    "\n",
    "train_data = data\n",
    "test_data = concatenate_datasets([anli_r1, anli_r2, anli_r3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a92dc2a-d1fd-41ee-9234-a2a3c6b820dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour sampler N exemples par classe\n",
    "def sample_per_class(dataset, num_samples=256):\n",
    "    # Obtenir un index unique pour chaque classe\n",
    "    class_indices = {label: [] for label in set(dataset['label'])}\n",
    "\n",
    "    # Accumuler les indices pour chaque classe\n",
    "    for index, label in enumerate(dataset['label']):\n",
    "        class_indices[label].append(index)\n",
    "\n",
    "    # Sélectionner aléatoirement num_samples indices pour chaque classe\n",
    "    import random\n",
    "    sampled_indices = [index for label, indices in class_indices.items()\n",
    "                       for index in random.sample(indices, num_samples)]\n",
    "\n",
    "    # Créer un nouveau dataset à partir des indices échantillonnés\n",
    "    sampled_dataset = dataset.select(sampled_indices)\n",
    "    return sampled_dataset\n",
    "\n",
    "# Appliquer la fonction\n",
    "train_data = sample_per_class(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5fb654-4d6a-4acf-9416-ac330db138d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    task_description = \"Determine if the hypothesis is true based on the premise.\"\n",
    "    inputs = [f\"Task: {task_description} Premise: {premise} Hypothesis: {hypothesis} Label (entailment, neutral, contradiction):\" for premise, hypothesis in zip(examples['premise'], examples['hypothesis'])]\n",
    "    # Tokeniser les inputs en batch\n",
    "    labels = examples['label']\n",
    "    model_inputs = tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=1000, add_special_tokens=True)\n",
    "    model_inputs[\"label\"] = labels\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5ab378-4214-40c0-9ea0-4dd1de84a1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the tokenization and preparation function\n",
    "train_data = train_data.map(preprocess_function, batched=True)\n",
    "test_data = test_data.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3869e9a-9f6d-4830-a7ea-9457e69bb4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    outputs, labels = eval_pred  # eval_pred is the tuple of predictions and labels returned by the model\n",
    "    \n",
    "    # Obtenir les logits des outputs\n",
    "    logits = outputs[0]\n",
    "    \n",
    "    # Appliquer la fonction Softmax pour obtenir les probabilités\n",
    "    probabilities = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    \n",
    "    # Prendre l'argmax des probabilités pour obtenir les prédictions\n",
    "    predictions = np.argmax(probabilities, axis=-1)\n",
    "    \n",
    "    # Calculate metrics, assuming 'average' as 'weighted' for handling multiclass classification\n",
    "    precision = precision_score(labels, predictions, average='weighted')\n",
    "    recall = recall_score(labels, predictions, average='weighted')\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    # Calculate accuracy for each class\n",
    "    class_accuracies = {}\n",
    "    for cls in range(3):\n",
    "        class_indices = (labels == cls)\n",
    "        class_accuracy = accuracy_score(labels[class_indices], predictions[class_indices])\n",
    "        class_accuracies[f\"accuracy_class_{cls}\"] = class_accuracy\n",
    "    \n",
    "    # Return a dictionary with the computed metrics\n",
    "    metrics = {\n",
    "        \"precision\": precision, \n",
    "        \"recall\": recall, \n",
    "        \"f1-score\": f1, \n",
    "        \"accuracy\": accuracy\n",
    "    }\n",
    "    \n",
    "    # Add class-wise accuracies to the metrics dictionary\n",
    "    metrics.update(class_accuracies)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea295cd-3a9e-4af0-b8cf-5ca768c60604",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#new code using SFTTrainer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Configuration des arguments de l'entraînement\n",
    "training_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    lr_scheduler_type=\"constant_with_warmup\",\n",
    "    warmup_ratio=0.08,\n",
    "    weight_decay=0.01, # if not, not stable\n",
    "    learning_rate=1e-4,\n",
    "    output_dir=\"outputs\",\n",
    "    optim='adafactor', # designed for T5\n",
    "    evaluation_strategy='no',\n",
    "    save_strategy=\"no\",\n",
    "    fp16=False,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    peft_config=lora_config,\n",
    "    args=training_args,\n",
    "    max_seq_length=1024,\n",
    "    data_collator=transformers.DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=10),\n",
    "    formatting_func=preprocess_function,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3b17d7-c790-42d4-9d45-b7b185976029",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0b8026-20ae-40d2-b0fb-ddcaf8390d26",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d695af-c1d8-4f86-bddd-d416516946bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046c2521-469f-4f2c-ba72-7c64448b3fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load ANLI test rounds\n",
    "anli_r1 = load_dataset(\"anli\", split=\"test_r1[:10%]\")\n",
    "anli_r2 = load_dataset(\"anli\", split=\"test_r2[:10%]\")\n",
    "anli_r3 = load_dataset(\"anli\", split=\"test_r3[:10%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383cea87-b82c-4588-8552-3cf7528b81cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process the ANLI dataset rounds\n",
    "def process_anli_data(dataset):\n",
    "    # Tokenize the data\n",
    "    dataset = dataset.map(preprocess_function, batched=True, remove_columns=[col for col in dataset.column_names if col not in ['premise', 'hypothesis', 'label']])\n",
    "    return dataset\n",
    "\n",
    "# Step 2: Process the data\n",
    "anli_r1 = process_anli_data(anli_r1)\n",
    "anli_r2 = process_anli_data(anli_r2)\n",
    "anli_r3 = process_anli_data(anli_r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6b1a0f-15b8-414f-a584-17a180d34af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define predict function with metrics\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "def predict_and_evaluate(dataset):\n",
    "    with torch.inference_mode():\n",
    "        predictions = trainer.evaluate(dataset)\n",
    "    print(\"Precision:\", predictions['eval_precision'])\n",
    "    print(\"Recall:\", predictions['eval_recall'])\n",
    "    print(\"F1-score:\", predictions['eval_f1-score'])\n",
    "    print(\"Accuracy:\", predictions['eval_accuracy'])\n",
    "    print(\"Accuracy entailment:\", predictions['eval_accuracy_class_0'])\n",
    "    print(\"Accuracy neutral:\", predictions['eval_accuracy_class_1'])\n",
    "    print(\"Accuracy contradiction:\", predictions['eval_accuracy_class_2'])\n",
    "\n",
    "# Step 4: Run predictions and compute metrics for each ANLI round\n",
    "print(\"Evaluating ANLI R1\")\n",
    "predict_and_evaluate(anli_r1)\n",
    "\n",
    "print(\"Evaluating ANLI R2\")\n",
    "predict_and_evaluate(anli_r2)\n",
    "\n",
    "print(\"Evaluating ANLI R3\")\n",
    "predict_and_evaluate(anli_r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b836594-baa9-402c-b3ff-390059b435c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vitaminc = load_dataset(\"Gameselo/monolingual-wideNLI\", split=\"test_vitaminc[:2%]\")\n",
    "test_vitaminc = process_anli_data(test_vitaminc)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Step 4: Run predictions and compute metrics for each ANLI round\n",
    "print(\"Evaluating VitaminC\")\n",
    "predict_and_evaluate(test_vitaminc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb806c11-7d58-4abd-9906-49f81a76e132",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('./best_model_mt5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
